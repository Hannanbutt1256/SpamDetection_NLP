{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacbca0c",
   "metadata": {},
   "source": [
    "# Data Acquisition & Exploration\n",
    "\n",
    "First, I imported polars as replacement of pandas here just for experimenting. Then read the csv file after that removed non-essential columns. Then converted `ham` and `spam` as `0` and `1` while coverting them to `int-8`.\n",
    "After that I labeled the columns for better readability\n",
    "\n",
    "## Note:\n",
    "\n",
    "The questions asked in notion are explained at the end of this notebook and bonus point question just before the serialization of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-1",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Importing the Polars library as a faster alternative to Pandas for data manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d6d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-2",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Reading the raw spam dataset from a CSV file. We use `latin-1` encoding to handle special characters common in SMS datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818a535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = pl.read_csv(\n",
    "    \"C:/Users/hanna/Documents/AI projects/SpamDetection_NLP/Data/spam.csv\",\n",
    "    encoding=\"latin-1\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-3",
   "metadata": {},
   "source": [
    "### Column Selection\n",
    "Filtering the dataset to keep only the relevant columns, as the original file contains several empty trailing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4de254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df_spam.select(df_spam.columns[:-3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-4",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "Mapping the target labels 'ham' and 'spam' to numeric values (0 and 1) and casting them to an efficient 8-bit integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0725e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df_spam.with_columns(\n",
    "    pl.col(\"v1\")\n",
    "      .replace({\"ham\": 0, \"spam\": 1})\n",
    "      .cast(pl.Int8)\n",
    "      .alias(\"label\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-5",
   "metadata": {},
   "source": [
    "### Cleaning Columns\n",
    "Removing the original label column after encoding it into a new 'label' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c1c5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df_spam.drop(\"v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-6",
   "metadata": {},
   "source": [
    "### Renaming Columns\n",
    "Renaming the text column to 'text' for clarity and consistency across the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13409f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df_spam.rename({\n",
    "    \"v2\": \"text\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-7",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "Displaying the processed dataframe to verify the structure and contents before moving to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "718b9859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_572, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>label</th></tr><tr><td>str</td><td>i8</td></tr></thead><tbody><tr><td>&quot;Go until jurong point, crazy..…</td><td>0</td></tr><tr><td>&quot;Ok lar... Joking wif u oni...&quot;</td><td>0</td></tr><tr><td>&quot;Free entry in 2 a wkly comp to…</td><td>1</td></tr><tr><td>&quot;U dun say so early hor... U c …</td><td>0</td></tr><tr><td>&quot;Nah I don&#x27;t think he goes to u…</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;This is the 2nd time we have t…</td><td>1</td></tr><tr><td>&quot;Will Ì_ b going to esplanade f…</td><td>0</td></tr><tr><td>&quot;Pity, * was in mood for that. …</td><td>0</td></tr><tr><td>&quot;The guy did some bitching but …</td><td>0</td></tr><tr><td>&quot;Rofl. Its true to its name&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_572, 2)\n",
       "┌─────────────────────────────────┬───────┐\n",
       "│ text                            ┆ label │\n",
       "│ ---                             ┆ ---   │\n",
       "│ str                             ┆ i8    │\n",
       "╞═════════════════════════════════╪═══════╡\n",
       "│ Go until jurong point, crazy..… ┆ 0     │\n",
       "│ Ok lar... Joking wif u oni...   ┆ 0     │\n",
       "│ Free entry in 2 a wkly comp to… ┆ 1     │\n",
       "│ U dun say so early hor... U c … ┆ 0     │\n",
       "│ Nah I don't think he goes to u… ┆ 0     │\n",
       "│ …                               ┆ …     │\n",
       "│ This is the 2nd time we have t… ┆ 1     │\n",
       "│ Will Ì_ b going to esplanade f… ┆ 0     │\n",
       "│ Pity, * was in mood for that. … ┆ 0     │\n",
       "│ The guy did some bitching but … ┆ 0     │\n",
       "│ Rofl. Its true to its name      ┆ 0     │\n",
       "└─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953edb0",
   "metadata": {},
   "source": [
    "# Pre‑processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-8",
   "metadata": {},
   "source": [
    "### Preprocessing Logic\n",
    "Setting up a text preprocessing pipeline: converts text to lowercase, removes punctuation, tokenizes, removes stop words, and applies lemmatization to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db2c9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    token = word_tokenize(text)\n",
    "    token = [t for t in token if t not in stop_words]\n",
    "    token = [lemmatizer.lemmatize(t) for t in token]\n",
    "    return \" \".join(token) \n",
    "\n",
    "\n",
    "df_spam = df_spam.with_columns(\n",
    "    pl.col('text').map_elements(preprocess, return_dtype=pl.Utf8).alias('text')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-9",
   "metadata": {},
   "source": [
    "### Preprocessing Execution\n",
    "After applying the preprocessing function to the entire dataset to clean the text data for modeling just checking it worked or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfb4a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_572, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>label</th></tr><tr><td>str</td><td>i8</td></tr></thead><tbody><tr><td>&quot;go jurong point crazy availabl…</td><td>0</td></tr><tr><td>&quot;ok lar joking wif u oni&quot;</td><td>0</td></tr><tr><td>&quot;free entry 2 wkly comp win fa …</td><td>1</td></tr><tr><td>&quot;u dun say early hor u c alread…</td><td>0</td></tr><tr><td>&quot;nah dont think go usf life aro…</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2nd time tried 2 contact u u å…</td><td>1</td></tr><tr><td>&quot;ì_ b going esplanade fr home&quot;</td><td>0</td></tr><tr><td>&quot;pity mood soany suggestion&quot;</td><td>0</td></tr><tr><td>&quot;guy bitching acted like id int…</td><td>0</td></tr><tr><td>&quot;rofl true name&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_572, 2)\n",
       "┌─────────────────────────────────┬───────┐\n",
       "│ text                            ┆ label │\n",
       "│ ---                             ┆ ---   │\n",
       "│ str                             ┆ i8    │\n",
       "╞═════════════════════════════════╪═══════╡\n",
       "│ go jurong point crazy availabl… ┆ 0     │\n",
       "│ ok lar joking wif u oni         ┆ 0     │\n",
       "│ free entry 2 wkly comp win fa … ┆ 1     │\n",
       "│ u dun say early hor u c alread… ┆ 0     │\n",
       "│ nah dont think go usf life aro… ┆ 0     │\n",
       "│ …                               ┆ …     │\n",
       "│ 2nd time tried 2 contact u u å… ┆ 1     │\n",
       "│ ì_ b going esplanade fr home    ┆ 0     │\n",
       "│ pity mood soany suggestion      ┆ 0     │\n",
       "│ guy bitching acted like id int… ┆ 0     │\n",
       "│ rofl true name                  ┆ 0     │\n",
       "└─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-10",
   "metadata": {},
   "source": [
    "### Detailed Inspection\n",
    "Printing the first 10 rows of the cleaned text to get a better sense of the results of the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90111b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go jurong point crazy available bugis n great world la e buffet cine got amore wat\n",
      "ok lar joking wif u oni\n",
      "free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry questionstd txt ratetcs apply 08452810075over18s\n",
      "u dun say early hor u c already say\n",
      "nah dont think go usf life around though\n",
      "freemsg hey darling 3 week word back id like fun still tb ok xxx std chgs send å150 rcv\n",
      "even brother like speak treat like aid patent\n",
      "per request melle melle oru minnaminunginte nurungu vettam set callertune caller press 9 copy friend callertune\n",
      "winner valued network customer selected receivea å900 prize reward claim call 09061701461 claim code kl341 valid 12 hour\n",
      "mobile 11 month u r entitled update latest colour mobile camera free call mobile update co free 08002986030\n"
     ]
    }
   ],
   "source": [
    "# Show first 10 rows fully\n",
    "for row in df_spam.head(10).select(\"text\").to_series():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-11",
   "metadata": {},
   "source": [
    "### Split Data Preparation\n",
    "Importing the necessary utilities for splitting the dataset into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3868b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-12",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Defining the feature set (X) and the target labels (y) from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f35f5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_spam[\"text\"]\n",
    "y = df_spam[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-13",
   "metadata": {},
   "source": [
    "### Train-Val-Test Split\n",
    "Splitting the data into training (70%), validation (10%), and testing (20%) sets using stratified sampling to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40b08c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 3900, Val: 568, Test: 1104\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.66, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-14",
   "metadata": {},
   "source": [
    "### Vectorization Imports\n",
    "Importing vectorization techniques to convert raw text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553175c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-15",
   "metadata": {},
   "source": [
    "### Bag-of-Words (BoW)\n",
    "Implementing a Bag-of-Words vectorizer with bigrams and a minimum frequency threshold to capture word frequency and simple context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a7031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-16",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "Applying TF-IDF (Term Frequency-Inverse Document Frequency) to weigh words based on their importance relative to the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ca8ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df= 2)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-17",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Loading pre-trained FastText embeddings to capture semantic meanings and handle out-of-vocabulary words better than sparse vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba22aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-18",
   "metadata": {},
   "source": [
    "### Embedding Logic\n",
    "Defining a function to aggregate word-level embeddings into a single document-level vector by calculating the mean of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aca288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(doc):\n",
    "    words = doc.split()\n",
    "    vecs = [word2vec_model[w] for w in words if w in word2vec_model]\n",
    "    if len(vecs) == 0:  # fallback for empty doc\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(vecs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-19",
   "metadata": {},
   "source": [
    "### Embedding Generation\n",
    "Converting the training, validation, and test sets into dense vectors using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178b5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_dense = np.array([document_vector(d) for d in X_train])\n",
    "X_val_dense = np.array([document_vector(d) for d in X_val])\n",
    "X_test_dense = np.array([document_vector(d) for d in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-20",
   "metadata": {},
   "source": [
    "### Evaluation Framework\n",
    "Defining a reusable evaluation function to train models and print performance metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e8daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model, X_tr, y_tr, X_te, y_te, description=\"Model\"):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_te, y_pred))\n",
    "    print(classification_report(y_te, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-21",
   "metadata": {},
   "source": [
    "### Naive Bayes Training\n",
    "Training and evaluating a Multinomial Naive Bayes model on both BoW and TF-IDF features to compare their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "701be819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Naive Bayes (BoW) ===\n",
      "Accuracy: 0.9753521126760564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       492\n",
      "           1       0.96      0.86      0.90        76\n",
      "\n",
      "    accuracy                           0.98       568\n",
      "   macro avg       0.97      0.92      0.94       568\n",
      "weighted avg       0.98      0.98      0.97       568\n",
      "\n",
      "\n",
      "=== Naive Bayes (TF-IDF) ===\n",
      "Accuracy: 0.9559859154929577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       492\n",
      "           1       1.00      0.67      0.80        76\n",
      "\n",
      "    accuracy                           0.96       568\n",
      "   macro avg       0.98      0.84      0.89       568\n",
      "weighted avg       0.96      0.96      0.95       568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "evaluate_model(nb_model, X_train_bow, y_train, X_val_bow, y_val, \"Naive Bayes (BoW)\")\n",
    "evaluate_model(nb_model, X_train_tfidf, y_train, X_val_tfidf, y_val, \"Naive Bayes (TF-IDF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-22",
   "metadata": {},
   "source": [
    "### Logistic Regression Training\n",
    "Training and evaluating a Logistic Regression model on the sparse BoW and TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0b80afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression (BoW) ===\n",
      "Accuracy: 0.971830985915493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       492\n",
      "           1       1.00      0.79      0.88        76\n",
      "\n",
      "    accuracy                           0.97       568\n",
      "   macro avg       0.98      0.89      0.93       568\n",
      "weighted avg       0.97      0.97      0.97       568\n",
      "\n",
      "\n",
      "=== Logistic Regression (TF-IDF) ===\n",
      "Accuracy: 0.9665492957746479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       492\n",
      "           1       0.98      0.76      0.86        76\n",
      "\n",
      "    accuracy                           0.97       568\n",
      "   macro avg       0.97      0.88      0.92       568\n",
      "weighted avg       0.97      0.97      0.96       568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression (sparse)\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "evaluate_model(lr_model, X_train_bow, y_train, X_val_bow, y_val, \"Logistic Regression (BoW)\")\n",
    "evaluate_model(lr_model, X_train_tfidf, y_train, X_val_tfidf, y_val, \"Logistic Regression (TF-IDF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-23",
   "metadata": {},
   "source": [
    "### Logistic Regression (Dense)\n",
    "Evaluating Logistic Regression performance when using dense Word2Vec/FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "882916cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression (Word2Vec) ===\n",
      "Accuracy: 0.9295774647887324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       492\n",
      "           1       0.79      0.64      0.71        76\n",
      "\n",
      "    accuracy                           0.93       568\n",
      "   macro avg       0.87      0.81      0.84       568\n",
      "weighted avg       0.93      0.93      0.93       568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_dense_model = LogisticRegression(max_iter=500)\n",
    "evaluate_model(lr_dense_model, X_train_dense, y_train, X_val_dense, y_val, \"Logistic Regression (Word2Vec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-24",
   "metadata": {},
   "source": [
    "### Generative Demo\n",
    "Building a Markov Chain on the training data to demonstrate a simple generative approach to text, showing how words follow one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53e5e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3-gram Markov Chain Generated Text ===\n",
      "ceive å500000 easter prize drawplease telephone 09041940223 claim 290305 prize transferred someone e\n",
      "leep wish great day full feeling better opportunity last thought babe love kiss urgent ur å500 guara\n",
      "a thanks talk saturday dear cherish brother role model k im leaving soon little 9 ok anyway need cha\n",
      "e india onionrs ltgt petrolrs ltgt beerrs ltgt shesil ltgt hello yeah ive got bath need hair ill com\n",
      "axx match startedindia ltgt 2 jokin oni lar ìï busy wun disturb ì_ guy go see movie side ok come n p\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_markov_chain(texts, n=3):\n",
    "    chain = defaultdict(list)\n",
    "    corpus = \" \".join(texts)\n",
    "    for i in range(len(corpus) - n):\n",
    "        ngram = corpus[i:i+n]\n",
    "        next_char = corpus[i+n]\n",
    "        chain[ngram].append(next_char)\n",
    "    return chain\n",
    "\n",
    "def generate_text(chain, length=100):\n",
    "    ngram = random.choice(list(chain.keys()))\n",
    "    result = ngram\n",
    "    for _ in range(length):\n",
    "        if ngram in chain:\n",
    "            next_char = random.choice(chain[ngram])\n",
    "            result += next_char\n",
    "            ngram = result[-len(ngram):]\n",
    "        else:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# Build chain on training data\n",
    "markov_chain = build_markov_chain(X_train, n=20)\n",
    "\n",
    "# Generate 5 sample texts\n",
    "print(\"\\n=== 3-gram Markov Chain Generated Text ===\")\n",
    "for i in range(5):\n",
    "    print(generate_text(markov_chain, length=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-25",
   "metadata": {},
   "source": [
    "### Final Evaluation\n",
    "Performing a final evaluation of the best-performing model (Naive Bayes with BoW) on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29f1ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Naives Bayes Model with BOW ===\n",
      "Accuracy: 0.9764492753623188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       956\n",
      "           1       0.94      0.88      0.91       148\n",
      "\n",
      "    accuracy                           0.98      1104\n",
      "   macro avg       0.96      0.94      0.95      1104\n",
      "weighted avg       0.98      0.98      0.98      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_bow_model = nb_model.fit(X_train_bow, y_train)\n",
    "y_pred = nb_model.predict(X_test_bow)\n",
    "print(f\"\\n=== Naives Bayes Model with BOW ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-26",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Saving the trained Naive Bayes model using Pickle for future deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce42cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"spam_bow_nb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nb_bow_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desc-27",
   "metadata": {},
   "source": [
    "### Vectorizer Serialization\n",
    "Saving the BoW vectorizer to ensure consistent preprocessing during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6045fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bow_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bow_vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a208e04",
   "metadata": {},
   "source": [
    "# Questions Asked in Notion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d29d3",
   "metadata": {},
   "source": [
    "### Overall understanding of teh data:\n",
    "\n",
    "overall this was an imbalanced dataset but it nature as we usually have more ham mails instead of spam.Same like Fraud detection dataset.\n",
    "\n",
    "Bag of Words works best with this dataset as word level count for spam dataset that is imbalanced help us get as much context we can. Both NB and LR works with BoW. But NB performs better in case of Recall and that i think key distinguishing factor in case of evaluation.\n",
    "\n",
    "TD-IDF suffers here in both NB and LR in case of Recall means it was unable to identify the spam messages out of total spam messages present. I think it happens because most of the spam messages contain the same spam-words repeating accross. So when we apply TD-IDF it has given the less weitage to them because it prefers rare words do its quality suffers\n",
    "\n",
    "In my case Word-Vec doesnot have a good result. Even it was the worst of all.It could because of 2 things mainly:\n",
    "1. Small dataset\n",
    "2. Or it may require DL techniques for better catching the context for overall better performance.\n",
    "\n",
    "### Final Words \n",
    "1. For our spam detection problem: Naive Bayes + BoW is the best choice\n",
    "\n",
    "2. Simple, fast, interpretable, and robust\n",
    "\n",
    "3. TF-IDF and Word2Vec only make sense if you plan to scale up dataset or move to neural/transformer-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842906fa",
   "metadata": {},
   "source": [
    "# Compare generative vs. discriminative performance:\n",
    "\n",
    "- Generative (NB) models estimate better at catching minority class (spam) even with small datasets.\n",
    "\n",
    "- Discriminative (LR) models estimate safer, avoid false positives, but more conservative, missing some spams.\n",
    "\n",
    "- On small, sparse datasets like yours, generative NB with BoW slightly outperforms discriminative LR in F1/recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2186d",
   "metadata": {},
   "source": [
    "# Discuss how N‑gram size and embedding choice affected results:\n",
    "- Unigrams (1‑gram) captures most spam tokens, e.g., “free”, “win”, “prize”\n",
    "\n",
    "- Adding bigrams helped detecting phrase-level spam patterns (“call now”, “win free ticket”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf43036",
   "metadata": {},
   "source": [
    "# Effect of Embedding Choice\n",
    "\n",
    "| Representation | Observed Effect in my Results                                                                   |\n",
    "| -------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| BoW            | Strongest recall, simple, directly counts spam tokens, robust for small datasets                  |\n",
    "| TF-IDF         | Damaged recall for spam, downweights frequent spam words → overly conservative                    |\n",
    "| Word2Vec       | Averaging embeddings **dilutes rare spam tokens**, LR cannot separate them → lowest F1 and recall |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd89b6",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Explainability\n",
    "\n",
    "| Model       | Speed                             | Memory                                    | Explainability                                                       |\n",
    "| ----------- | --------------------------------- | ----------------------------------------- | -------------------------------------------------------------------- |\n",
    "| NB BoW      | ✅ Very fast to train & predict    | ✅ Low memory (sparse counts)              | ✅ Highly interpretable (word probabilities for spam)                 |\n",
    "| NB TF-IDF   | ✅ Fast                            | ✅ Slightly higher (sparse TF-IDF vectors) | ✅ Probabilities still interpretable                                  |\n",
    "| LR BoW      | ✅ Fast                            | ✅ Moderate                                | ✅ Coefficients interpretable (weight per token)                      |\n",
    "| LR TF-IDF   | ✅ Fast                            | ✅ Moderate                                | ✅ Coefficients interpretable, harder to interpret downweighted words |\n",
    "| LR Word2Vec | ⚠ Slower (embeddings + averaging) | ⚠ Dense vectors → higher memory           | ⚠ Low explainability; difficult to trace which words triggered spam  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3a34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
